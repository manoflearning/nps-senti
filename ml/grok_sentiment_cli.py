# ml/grok_sentiment_cli.py
from __future__ import annotations

import argparse
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .grok_client import GrokClient


logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s: %(message)s",
)


# ---------- í…ìŠ¤íŠ¸ & ë©”íƒ€ ì¶”ì¶œ ----------


@dataclass
class TextAndMeta:
    text: str
    meta: Dict[str, Any]


def extract_text_and_meta(obj: Dict[str, Any]) -> TextAndMeta:
    """
    ë‹¤ì–‘í•œ ì†ŒìŠ¤(dcinside, bobaedream, youtube, gdelt, etc.)ë¥¼ ê³µí†µ í¬ë§·ìœ¼ë¡œ ë§ì¶°ì„œ
    GrokClient.analyze_sentiment ì— ë„˜ê¸°ê¸° ìœ„í•œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë§Œë“ ë‹¤.
    """
    source = obj.get("source") or ""
    lang = obj.get("lang") or None
    published_at = obj.get("published_at") or None
    identifier = obj.get("id") or obj.get("_id") or None

    # doc_type ì¶”ë¡ 
    doc_type = obj.get("doc_type")
    if not doc_type:
        if source == "youtube":
            doc_type = "video"
        elif source in ("naver_news", "news", "gdelt"):
            doc_type = "article"
        else:
            doc_type = "post"

    text_candidates: List[Optional[str]] = []

    # 1) í¬ëŸ¼ë¥˜(ë””ì‹œ, ë³´ë°° ë“±): combined_text
    if "combined_text" in obj:
        text_candidates.append(obj.get("combined_text"))

    # 2) ì´ì „ ë²„ì „ ì „ì²˜ë¦¬: text_clean
    if "text_clean" in obj:
        text_candidates.append(obj.get("text_clean"))

    # 3) ìœ íŠœë¸Œ (ìµœì‹  minimal ë²„ì „): title + description ì¡°í•©
    if source == "youtube":
        title = (obj.get("title") or "").strip()
        desc = (obj.get("description") or "").strip()
        if title and desc:
            text_candidates.append(f"{title}\n\n{desc}")
        elif title:
            text_candidates.append(title)

    # ğŸ”¥ 4) GDELT ê¸°ì‚¬: title + text ì¡°í•©
    if source == "gdelt":
        title = (obj.get("title") or "").strip()
        body = (obj.get("text") or "").strip()
        if title and body:
            text_candidates.append(f"{title}\n\n{body}")
        elif body:
            text_candidates.append(body)

    # 5) ëŒ“ê¸€ë§Œ ìˆëŠ” ê²½ìš°: comment_text
    if "comment_text" in obj:
        text_candidates.append(obj.get("comment_text"))

    # 6) ì¼ë°˜ ê¸°ì‚¬/í…ìŠ¤íŠ¸: text, body, content ë“±
    for key in ("text", "body", "content"):
        if key in obj:
            text_candidates.append(obj.get(key))

    # 7) ê·¸ë˜ë„ ì—†ìœ¼ë©´: titleë§Œì´ë¼ë„
    if "title" in obj:
        text_candidates.append(obj.get("title"))

    text = ""
    for cand in text_candidates:
        if cand and isinstance(cand, str) and cand.strip():
            text = cand.strip()
            break

    if not text:
        logger.warning(
            "[WARN] id=%s ì— í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŒ, ì œëª©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.", identifier
        )
        title_fallback = (obj.get("title") or "").strip()
        text = title_fallback

    meta: Dict[str, Any] = {
        "id": identifier,
        "source": source,
        "doc_type": doc_type,
        "lang": lang,
        "published_at": published_at,
    }

    # ğŸ”¥ GDELTì˜ sourcecountryë„ ë©”íƒ€ì— í¬í•¨
    if "sourcecountry" in obj and obj.get("sourcecountry"):
        meta["sourcecountry"] = obj.get("sourcecountry")

    return TextAndMeta(text=text, meta=meta)


# ---------- JSONL ì…ì¶œë ¥ ----------


def read_jsonl(path: str | Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {p}")

    records: List[Dict[str, Any]] = []
    with p.open("r", encoding="utf-8") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError as exc:
                logger.warning(
                    "[WARN] ë¼ì¸ %d JSON íŒŒì‹± ì‹¤íŒ¨, ìŠ¤í‚µ: %s", line_no, str(exc)
                )
                continue
            records.append(obj)
            if limit is not None and len(records) >= limit:
                break

    logger.info("[INFO] ì…ë ¥ì—ì„œ %dê°œ ë ˆì½”ë“œ ë¡œë“œ", len(records))
    return records


def write_jsonl(path: str | Path, records: List[Dict[str, Any]]) -> None:
    p = Path(path)
    if p.parent and not p.parent.exists():
        p.parent.mkdir(parents=True, exist_ok=True)

    with p.open("w", encoding="utf-8") as f:
        for obj in records:
            f.write(json.dumps(obj, ensure_ascii=False))
            f.write("\n")


# ---------- ê°œë³„ ë ˆì½”ë“œ ë¶„ì„ ----------


def analyze_one(
    client: GrokClient,
    index: int,
    obj: Dict[str, Any],
) -> Tuple[int, Dict[str, Any]]:
    """
    í•œ ë ˆì½”ë“œì— ëŒ€í•´ Grok ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ ,
    ì›ë³¸ + sentiment í•„ë“œê°€ í•©ì³ì§„ dictë¥¼ ë°˜í™˜í•œë‹¤.
    """
    try:
        tm = extract_text_and_meta(obj)
        result = client.analyze_sentiment(tm.text, tm.meta)
    except Exception as exc:
        logger.warning("[WARN] ë ˆì½”ë“œ index=%d ë¶„ì„ ì‹¤íŒ¨: %s", index, repr(exc))
        # ì•ˆì „í•œ fallback (ê·œì¹™ì— ë§ê²Œ ë¬´ê´€ ì²˜ë¦¬)
        result = {
            "is_related": False,
            "negative": 0.0,
            "neutral": 0.0,
            "positive": 0.0,
            "label": "ë¬´ê´€",
            "explanation": "êµ­ë¯¼ì—°ê¸ˆê³¼ ê´€ë ¨ ì—†ìŒ",
        }

    merged = {**obj, **result}
    return index, merged


# ---------- ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ----------


def process_file(
    input_path: str | Path,
    output_path: str | Path,
    limit: Optional[int] = None,
    workers: int = 1,
) -> None:
    records = read_jsonl(input_path, limit=limit)
    total = len(records)
    if total == 0:
        logger.warning("[WARN] ì…ë ¥ì— ìœ íš¨í•œ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("[INFO] ì´ %dê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì˜ˆì •, workers=%d", total, workers)

    client = GrokClient()

    results: Dict[int, Dict[str, Any]] = {}
    processed = 0

    if workers <= 1:
        for idx, obj in enumerate(records):
            _, merged = analyze_one(client, idx, obj)
            results[idx] = merged
            processed += 1
            if processed % 10 == 0 or processed == total:
                logger.info("[INFO] ì²˜ë¦¬ ì™„ë£Œ: %d/%d", processed, total)
    else:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(analyze_one, client, idx, obj): idx
                for idx, obj in enumerate(records)
            }

            for future in as_completed(futures):
                idx = futures[future]
                try:
                    _, merged = future.result()
                except Exception as exc:
                    logger.warning(
                        "[WARN] index=%d future ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: %s", idx, repr(exc)
                    )
                    merged = {
                        **records[idx],
                        "is_related": False,
                        "negative": 0.0,
                        "neutral": 0.0,
                        "positive": 0.0,
                        "label": "ë¬´ê´€",
                        "explanation": "êµ­ë¯¼ì—°ê¸ˆê³¼ ê´€ë ¨ ì—†ìŒ",
                    }
                results[idx] = merged
                processed += 1
                if processed % 10 == 0 or processed == total:
                    logger.info("[INFO] ì²˜ë¦¬ ì™„ë£Œ: %d/%d", processed, total)

    # ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì„œ ì¶œë ¥
    ordered_records = [results[i] for i in range(total)]
    write_jsonl(output_path, ordered_records)
    logger.info("[INFO] ëª¨ë“  ì‘ì—… ì™„ë£Œ. ê²°ê³¼: %s", Path(output_path).resolve())


# ---------- CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ----------


def main(argv: Optional[List[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description="êµ­ë¯¼ì—°ê¸ˆ ì˜¨ë¼ì¸ ì—¬ë¡ (ëŒ“ê¸€/ê²Œì‹œê¸€/ì˜ìƒ)ì— ëŒ€í•œ Grok-4-fast ê°ì„±ë¶„ì„ CLI"
    )
    parser.add_argument(
        "--input",
        "-i",
        required=True,
        help="ì…ë ¥ JSONL ê²½ë¡œ (ì˜ˆ: preprocess/preprocessing_data/youtube_preprocessed_minimal.jsonl)",
    )
    parser.add_argument(
        "--output",
        "-o",
        required=True,
        help="ì¶œë ¥ JSONL ê²½ë¡œ (ì˜ˆ: sentiment_output_data/youtube_sentiment.jsonl)",
    )
    parser.add_argument(
        "--limit",
        "-n",
        type=int,
        default=None,
        help="ì²˜ë¦¬í•  ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ (ë””ë²„ê·¸ìš©, ê¸°ë³¸: ì „ì²´)",
    )
    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        default=1,
        help="ë™ì‹œ ìš”ì²­ì— ì‚¬ìš©í•  ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 1)",
    )

    args = parser.parse_args(argv)

    process_file(
        input_path=args.input,
        output_path=args.output,
        limit=args.limit,
        workers=args.workers,
    )


if __name__ == "__main__":
    main()
