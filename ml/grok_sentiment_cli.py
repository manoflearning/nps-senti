# ml/grok_sentiment_cli.py
from __future__ import annotations

import argparse
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .grok_client import GrokClient #, DCINSIDE_NPS_PATTERN  # íŒ¨í„´ import


logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s: %(message)s",
)


# ---------- í…ìŠ¤íŠ¸ & ë©”íƒ€ ì¶”ì¶œ ----------


@dataclass
class TextAndMeta:
    text: str
    meta: Dict[str, Any]


def extract_text_and_meta(obj: Dict[str, Any]) -> TextAndMeta:
    """
    ë‹¤ì–‘í•œ ì†ŒìŠ¤(dcinside, bobaedream, youtube, gdelt, etc.)ë¥¼ ê³µí†µ í¬ë§·ìœ¼ë¡œ ë§ì¶°ì„œ
    GrokClient.analyze_sentiment ì— ë„˜ê¸°ê¸° ìœ„í•œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë§Œë“ ë‹¤.

    ğŸ”¥ ë³€ê²½ í•µì‹¬:
      - doc_type == "post": title + "\n\n" + text/body/content
      - doc_type == "comment": title + "\n\n" + text/body/content + "\n\n" + comment_text
      - dcinside: ì§§ì€ í…ìŠ¤íŠ¸ë„ ê·¸ëŒ€ë¡œ (ìµœì†Œ ê¸¸ì´ í•„í„° off), í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì‚¬ì „ ë¬´ê´€ (íš¨ìœ¨ì„± â†‘)
      - ë‹¤ë¥¸ ì†ŒìŠ¤: min_len=5
    """
    source = obj.get("source") or ""
    lang = obj.get("lang") or None
    published_at = obj.get("published_at") or None
    identifier = obj.get("id") or obj.get("_id") or None
    doc_type = obj.get("doc_type") or "post"  # ê¸°ë³¸ post

    title = (obj.get("title") or "").strip()
    text_body = ""
    for key in ("text", "body", "content", "combined_text", "text_clean"):
        if key in obj:
            text_body = (obj.get(key) or "").strip()
            break

    comment_text = (obj.get("comment_text") or "").strip()

    # âœ… doc_typeë³„ í…ìŠ¤íŠ¸ ì¡°í•©
    if doc_type == "post":
        text = f"{title}\n\n{text_body}" if title and text_body else title or text_body
    elif doc_type == "comment":
        text = f"{title}\n\n{text_body}\n\n{comment_text}" if title and text_body and comment_text else f"{title}\n\n{text_body or comment_text}"
    else:  # ê¸°íƒ€ (video, article ë“±)
        text = f"{title}\n\n{text_body}" if title else text_body

    if not text:
        logger.warning(
            "[WARN] id=%s ì— í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŒ, ì œëª©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.", identifier
        )
        text = title

    # # âœ… dcinside: í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì‚¬ì „ ë¬´ê´€ ì²˜ë¦¬ (íš¨ìœ¨ì„±: API í˜¸ì¶œ í”¼í•¨)
    # if "dcinside" in source.lower():
    #     if not DCINSIDE_NPS_PATTERN.search(text):
    #         logger.info("[INFO] dcinsideì§€ë§Œ NPS í‚¤ì›Œë“œ ì—†ìŒ: ë¬´ê´€ ì‚¬ì „ ì²˜ë¦¬. text='%s'", text[:50])
    #         text = ""  # ë¬´ê´€ íŠ¸ë¦¬ê±°
    # else:
    # ë‹¤ë¥¸ ì†ŒìŠ¤: ì§§ì€ í…ìŠ¤íŠ¸ fallback
    min_len = 5
    if len(text) < min_len:
        logger.warning(
            "[WARN] id=%s í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ (len=%d), ë¬´ê´€ ì²˜ë¦¬.",
            identifier,
            len(text),
        )
        text = ""

    meta: Dict[str, Any] = {
        "id": identifier,
        "source": source,
        "doc_type": doc_type,
        "lang": lang,
        "published_at": published_at,
    }

    # GDELTì˜ sourcecountryë„ ë©”íƒ€ì— í¬í•¨
    if "sourcecountry" in obj and obj.get("sourcecountry"):
        meta["sourcecountry"] = obj.get("sourcecountry")

    return TextAndMeta(text=text, meta=meta)


# ---------- JSONL ì…ì¶œë ¥ ----------


def read_jsonl(path: str | Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {p}")

    records: List[Dict[str, Any]] = []
    with p.open("r", encoding="utf-8") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError as exc:
                logger.warning(
                    "[WARN] ë¼ì¸ %d JSON íŒŒì‹± ì‹¤íŒ¨, ìŠ¤í‚µ: %s", line_no, str(exc)
                )
                continue
            records.append(obj)
            if limit is not None and len(records) >= limit:
                break

    logger.info("[INFO] ì…ë ¥ì—ì„œ %dê°œ ë ˆì½”ë“œ ë¡œë“œ", len(records))
    return records


def write_jsonl(path: str | Path, records: List[Dict[str, Any]]) -> None:
    p = Path(path)
    if p.parent and not p.parent.exists():
        p.parent.mkdir(parents=True, exist_ok=True)

    with p.open("w", encoding="utf-8") as f:
        for obj in records:
            f.write(json.dumps(obj, ensure_ascii=False))
            f.write("\n")


# ---------- ê°œë³„ ë ˆì½”ë“œ ë¶„ì„ ----------


def analyze_one(
    client: GrokClient,
    index: int,
    obj: Dict[str, Any],
) -> Tuple[int, Dict[str, Any]]:
    """
    í•œ ë ˆì½”ë“œì— ëŒ€í•´ Grok ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ ,
    ì›ë³¸ + sentiment í•„ë“œê°€ í•©ì³ì§„ dictë¥¼ ë°˜í™˜í•œë‹¤.
    """
    try:
        tm = extract_text_and_meta(obj)
        result = client.analyze_sentiment(tm.text, tm.meta)
    except Exception as exc:
        logger.warning("[WARN] ë ˆì½”ë“œ index=%d ë¶„ì„ ì‹¤íŒ¨: %s", index, repr(exc))
        # ì•ˆì „í•œ fallback (ê·œì¹™ì— ë§ê²Œ ë¬´ê´€ ì²˜ë¦¬)
        result = {
            "is_related": False,
            "negative": 0.0,
            "neutral": 0.0,
            "positive": 0.0,
            "label": "ë¬´ê´€",
            "explanation": "êµ­ë¯¼ì—°ê¸ˆê³¼ ê´€ë ¨ ì—†ìŒ",
        }

    merged = {**obj, **result}
    return index, merged


# ---------- ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ----------


def process_file(
    input_path: str | Path,
    output_path: str | Path,
    limit: Optional[int] = None,
    workers: int = 4,  # ìˆ˜ì •: ê¸°ë³¸ 4
) -> None:
    records = read_jsonl(input_path, limit=limit)
    total = len(records)
    if total == 0:
        logger.warning("[WARN] ì…ë ¥ì— ìœ íš¨í•œ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("[INFO] ì´ %dê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì˜ˆì •, workers=%d", total, workers)

    client = GrokClient()

    results: Dict[int, Dict[str, Any]] = {}
    processed = 0
    success_count = 0  # ì¶”ê°€: ì„±ê³µë¥  ë¡œê¹…

    if workers <= 1:
        for idx, obj in enumerate(records):
            _, merged = analyze_one(client, idx, obj)
            results[idx] = merged
            processed += 1
            success_count += 1 if merged.get("is_related") else 0  # ì˜ˆì‹œ
            if processed % 10 == 0 or processed == total:
                logger.info(
                    "[INFO] ì²˜ë¦¬ ì™„ë£Œ: %d/%d (ì„±ê³µë¥ : %.2f%%)",
                    processed,
                    total,
                    (success_count / processed * 100) if processed > 0 else 0,
                )
    else:
        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(analyze_one, client, idx, obj): idx
                for idx, obj in enumerate(records)
            }

            for future in as_completed(futures):
                idx = futures[future]
                try:
                    _, merged = future.result()
                    success_count += 1 if merged.get("is_related") else 0
                except Exception as exc:
                    logger.warning(
                        "[WARN] index=%d future ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: %s", idx, repr(exc)
                    )
                    merged = {
                        **records[idx],
                        "is_related": False,
                        "negative": 0.0,
                        "neutral": 0.0,
                        "positive": 0.0,
                        "label": "ë¬´ê´€",
                        "explanation": "êµ­ë¯¼ì—°ê¸ˆê³¼ ê´€ë ¨ ì—†ìŒ",
                    }
                results[idx] = merged
                processed += 1
                if processed % 10 == 0 or processed == total:
                    logger.info(
                        "[INFO] ì²˜ë¦¬ ì™„ë£Œ: %d/%d (ì„±ê³µë¥ : %.2f%%)",
                        processed,
                        total,
                        (success_count / processed * 100) if processed > 0 else 0,
                    )

    # ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì„œ ì¶œë ¥
    ordered_records = [results[i] for i in range(total)]
    write_jsonl(output_path, ordered_records)
    logger.info("[INFO] ëª¨ë“  ì‘ì—… ì™„ë£Œ. ê²°ê³¼: %s", Path(output_path).resolve())


# ---------- CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ----------


def main(argv: Optional[List[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description="êµ­ë¯¼ì—°ê¸ˆ ì˜¨ë¼ì¸ ì—¬ë¡ (ëŒ“ê¸€/ê²Œì‹œê¸€/ì˜ìƒ)ì— ëŒ€í•œ Grok-4-fast ê°ì„±ë¶„ì„ CLI"
    )
    parser.add_argument(
        "--input",
        "-i",
        required=True,
        help="ì…ë ¥ JSONL ê²½ë¡œ (ì˜ˆ: preprocess/preprocessing_data/youtube_preprocessed_minimal.jsonl)",
    )
    parser.add_argument(
        "--output",
        "-o",
        required=True,
        help="ì¶œë ¥ JSONL ê²½ë¡œ (ì˜ˆ: sentiment_output_data/youtube_sentiment.jsonl)",
    )
    parser.add_argument(
        "--limit",
        "-n",
        type=int,
        default=None,
        help="ì²˜ë¦¬í•  ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ (ë””ë²„ê·¸ìš©, ê¸°ë³¸: ì „ì²´)",
    )
    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        default=4,  # ìˆ˜ì •: ê¸°ë³¸ 4
        help="ë™ì‹œ ìš”ì²­ì— ì‚¬ìš©í•  ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 4)",
    )

    args = parser.parse_args(argv)

    process_file(
        input_path=args.input,
        output_path=args.output,
        limit=args.limit,
        workers=args.workers,
    )


if __name__ == "__main__":
    main()