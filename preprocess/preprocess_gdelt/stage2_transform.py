# preprocess/preprocess_gdelt/stage2_transform.py
from __future__ import annotations

from datetime import datetime, timezone
from typing import List, Optional, Dict
import logging
import re
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

from .stage1_models_io import RawGdeltArticle, FlattenedGdeltArticle
import difflib


logger = logging.getLogger(__name__)


# ---------- ë‚ ì§œ ì²˜ë¦¬ ----------


def normalize_iso_utc(s: Optional[str]) -> Optional[str]:
    """
    published_atì„ ìµœëŒ€í•œ 'YYYY-MM-DDTHH:MM:SSZ' í˜•íƒœë¡œ ë§ì¶˜ë‹¤.
    ì‹¤íŒ¨í•˜ë©´ None.
    """
    if not s:
        return None
    s = s.strip()
    if not s:
        return None

    try:
        if s.endswith("Z"):
            dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
        else:
            dt = datetime.fromisoformat(s)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
        dt_utc = dt.astimezone(timezone.utc)
        return dt_utc.replace(microsecond=0).isoformat().replace("+00:00", "Z")
    except Exception:
        # ISO íŒŒì‹±ì´ ì•ˆ ë˜ë©´ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•Šê³  None ë¦¬í„´
        return None


def choose_published_at(
    published_at: Optional[str],
    seendate: Optional[str],
) -> Optional[str]:
    """
    ìµœì¢… published_at ì„ íƒ ìš°ì„ ìˆœìœ„:
      1) published_at (ì œëŒ€ë¡œ ëœ ISOë©´ UTCë¡œ ì •ê·œí™”)
      2) seendate (ë§ˆì°¬ê°€ì§€)
    """
    norm = normalize_iso_utc(published_at)
    if norm is not None:
        return norm

    norm2 = normalize_iso_utc(seendate)
    if norm2 is not None:
        return norm2

    # ë‘˜ ë‹¤ íŒŒì‹± ì•ˆ ë˜ë©´ ì›ë³¸ published_atì´ë¼ë„ ëŒë ¤ì¤Œ
    return (published_at or seendate or None)


# ---------- í…ìŠ¤íŠ¸ í´ë¦¬ë‹ ----------

TAIL_PATTERNS = [
    "all rights reserved",
    "ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€",
    "Â©",
]


def clean_text(raw_text: str) -> str:
    """
    GDELT ë‰´ìŠ¤ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í´ë¦¬ë‹:
      - ì‚¬ì´íŠ¸ footer/ì €ì‘ê¶Œ ì•ˆë‚´ ì¼ë¶€ ì œê±° (íŒ¨í„´ ê¸°ë°˜)
      - ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    """
    if not raw_text:
        return ""

    text = raw_text.replace("\r\n", "\n").replace("\r", "\n")

    lower_text = text.lower()
    cut_pos = None
    for pat in TAIL_PATTERNS:
        idx = lower_text.find(pat.lower())
        if idx != -1:
            if cut_pos is None or idx < cut_pos:
                cut_pos = idx
    if cut_pos is not None and cut_pos > 0:
        text = text[:cut_pos]

    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]{2,}", " ", text)
    return text.strip()


# ---------- ì¤‘ë³µ ì²˜ë¦¬ ìœ í‹¸ ----------

TITLE_NORM_SPACE_RE = re.compile(r"\s+")


def normalize_title_for_key(title: str) -> str:
    """
    ì¤‘ë³µ ì œê±°ìš© ì œëª© ì •ê·œí™”:
      - ë’¤ì— ë¶™ì€ ë§¤ì²´ëª…/ì‚¬ì´íŠ¸ëª… ì˜ë¼ë‚´ê¸°
      - ì†Œë¬¸ì + ê³µë°± ì¶•ì†Œ
    ì˜ˆ)
      'Government shutdown: what closes - NPR' ->
      'government shutdown: what closes'
    """
    t = (title or "").strip()

    for sep in (" - ", "ï½œ", " | ", "|"):
        if sep in t:
            t = t.split(sep)[0]

    t = t.lower()
    t = TITLE_NORM_SPACE_RE.sub(" ", t)
    return t.strip()


def normalize_url_for_key(url: str) -> str:
    """
    URL ì •ê·œí™”:
      - scheme ì œê±° (http/https)
      - host ì†Œë¬¸ì
      - path ë ìŠ¬ë˜ì‹œ ì œê±°
      - utm_*, fbclid, gclid ë“± ì¶”ì ìš© ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
      - fragment ì œê±°
    """
    if not url:
        return ""

    u = url.strip()
    try:
        parsed = urlparse(u)
    except Exception:
        return u.lower()

    netloc = (parsed.netloc or "").lower()
    path = (parsed.path or "").rstrip("/")

    keep_pairs = []
    if parsed.query:
        for k, v in parse_qsl(parsed.query, keep_blank_values=True):
            kl = k.lower()
            if kl.startswith("utm_"):
                continue
            if kl in {"fbclid", "gclid", "mc_cid", "mc_eid"}:
                continue
            keep_pairs.append((k, v))
    query = urlencode(keep_pairs, doseq=True)

    normalized = urlunparse(("", netloc, path, "", query, ""))
    return normalized


# ---------- ì¤‘ë³µ ì œê±° í•µì‹¬ ----------


def deduplicate_records(
    records: List[FlattenedGdeltArticle],
) -> List[FlattenedGdeltArticle]:
    """
    GDELT ê¸°ì‚¬ ì¤‘ë³µ ì œê±° (ê°•í™” ë²„ì „).

    ì „ëµ:
      1) ìš°ì„  (lang, normalized_title) ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ì„ ë§Œë“ ë‹¤.
      2) ê·¸ë£¹ ì•ˆì—ì„œ text ìœ ì‚¬ë„(SequenceMatcher ë¹„ìœ¨)ê°€ 0.995 ì´ìƒì´ë©´
         ì‚¬ì‹¤ìƒ ê°™ì€ ê¸°ì‚¬ë¡œ ë³´ê³  1ê°œë§Œ ë‚¨ê¸´ë‹¤.
      3) ê°™ì€ ê¸°ì‚¬ ê·¸ë£¹ ì•ˆì—ì„œëŠ”
         - text ê¸¸ì´ê°€ ë” ê¸´ ê²ƒ
         - ê·¸ ë‹¤ìŒìœ¼ë¡œ published_atì´ ë” ìµœì‹ ì¸ ê²ƒ
         ì„ ìš°ì„  ì„ íƒí•œë‹¤.

    ì´ë ‡ê²Œ í•˜ë©´
      - 2296/2297ì²˜ëŸ¼ ì œëª©/ë‚´ìš©ì´ ê±°ì˜ ê°™ì€ ê¸°ì‚¬ì˜ ì¤‘ë³µì„ ì¡ìœ¼ë©´ì„œ
      - ì œëª©ë§Œ ê°™ê³  ë‚´ìš©ì´ ë‹¤ë¥¸ ê±´ ê·¸ëŒ€ë¡œ ì—¬ëŸ¬ ê°œ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.
    """

    from collections import defaultdict

    def normalize_title_for_key(title: str) -> str:
        t = (title or "").strip()
        for sep in (" - ", "ï½œ", " | ", "|"):
            if sep in t:
                t = t.split(sep)[0]
        t = t.lower()
        t = TITLE_NORM_SPACE_RE.sub(" ", t)
        return t.strip()

    def normalize_url_for_key(url: str) -> str:
        if not url:
            return ""
        u = url.strip()
        try:
            parsed = urlparse(u)
        except Exception:
            return u.lower()

        netloc = (parsed.netloc or "").lower()
        path = (parsed.path or "").rstrip("/")

        keep_pairs = []
        if parsed.query:
            for k, v in parse_qsl(parsed.query, keep_blank_values=True):
                kl = k.lower()
                if kl.startswith("utm_"):
                    continue
                if kl in {"fbclid", "gclid", "mc_cid", "mc_eid"}:
                    continue
                keep_pairs.append((k, v))
        query = urlencode(keep_pairs, doseq=True)

        normalized = urlunparse(("", netloc, path, "", query, ""))
        return normalized

    def parse_dt(s: Optional[str]) -> Optional[datetime]:
        if not s:
            return None
        s = s.strip()
        try:
            if s.endswith("Z"):
                s2 = s.replace("Z", "+00:00")
            else:
                s2 = s
            return datetime.fromisoformat(s2)
        except Exception:
            return None

    def choose_better(a: FlattenedGdeltArticle, b: FlattenedGdeltArticle) -> FlattenedGdeltArticle:
        # 1) text ê¸¸ì´ê°€ ê¸´ ê²ƒ ìš°ì„ 
        len_a = len(a.text or "")
        len_b = len(b.text or "")
        if len_b > len_a:
            winner, loser = b, a
        elif len_a > len_b:
            winner, loser = a, b
        else:
            # 2) ê¸¸ì´ê°€ ê°™ìœ¼ë©´ published_at ë” ìµœì‹ ì¸ ìª½
            da = parse_dt(a.published_at)
            db = parse_dt(b.published_at)
            if db and (not da or db > da):
                winner, loser = b, a
            else:
                winner, loser = a, b
        return winner

    # 1ë‹¨ê³„: (lang, normalized_title) ë¡œ ê·¸ë£¹í•‘
    groups: Dict[tuple, List[FlattenedGdeltArticle]] = defaultdict(list)
    for rec in records:
        lang_norm = (rec.lang or "").strip().lower()
        title_norm = normalize_title_for_key(rec.title or "")
        if lang_norm and title_norm:
            key = ("title", lang_norm, title_norm)
        elif rec.url:
            key = ("url", normalize_url_for_key(rec.url))
        else:
            key = ("id", rec.id)
        groups[key].append(rec)

    deduped: List[FlattenedGdeltArticle] = []
    total_merged = 0

    # 2ë‹¨ê³„: ê° ê·¸ë£¹ ì•ˆì—ì„œ text ìœ ì‚¬ë„ ê¸°ë°˜ dedup
    for key, recs in groups.items():
        selected: List[FlattenedGdeltArticle] = []
        for rec in recs:
            merged = False
            for i, kept in enumerate(selected):
                sim = difflib.SequenceMatcher(None, kept.text or "", rec.text or "").ratio()
                # ğŸ”¥ ê±°ì˜ ì™„ì „íˆ ê°™ì€ ê¸°ì‚¬ë©´ ê°™ì€ ê²ƒìœ¼ë¡œ ë³¸ë‹¤
                if sim >= 0.995:
                    better = choose_better(kept, rec)
                    selected[i] = better
                    total_merged += 1
                    merged = True
                    break
            if not merged:
                selected.append(rec)
        deduped.extend(selected)

    if total_merged > 0:
        logger.info(
            "[INFO] GDELT ì¤‘ë³µ ì œê±° (ì œëª©+í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜): "
            "ì›ë³¸ %dê°œ â†’ ì¤‘ë³µ ë³‘í•© %dê°œ â†’ ìµœì¢… %dê°œ",
            len(records),
            total_merged,
            len(deduped),
        )
    else:
        logger.info("[INFO] GDELT ì¤‘ë³µ ì œê±° ê²°ê³¼: ë³‘í•©ëœ ì¤‘ë³µ ì—†ìŒ (ì›ë³¸ %dê°œ)", len(records))

    return deduped


# ---------- Raw â†’ Flattened ----------


def flatten_article(
    raw: RawGdeltArticle,
    min_length: int = 0,
    max_length: Optional[int] = None,
) -> Optional[FlattenedGdeltArticle]:
    """
    RawGdeltArticle í•˜ë‚˜ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ FlattenedGdeltArticle ë¡œ ë³€í™˜.
    text ê¸¸ì´ ê¸°ì¤€(min_length, max_length)ì— ê±¸ë¦¬ë©´ None ë°˜í™˜.
    """
    title = (raw.title or "").strip()
    text_clean = clean_text(raw.text or "")
    length = len(text_clean)

    if min_length and length < min_length:
        return None
    if max_length is not None and length > max_length:
        return None

    published_at_iso = choose_published_at(raw.published_at, raw.seendate)

    return FlattenedGdeltArticle(
        id=raw.id,
        source=raw.source or "gdelt",
        lang=raw.lang or "en",
        title=title,
        text=text_clean,
        published_at=published_at_iso,
        url=raw.url,
    )
