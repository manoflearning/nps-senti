"""
Deduplicate a JSONL file of GDELT-preprocessed rows using:

1) Exact-text dedup (fast, hard duplicates)
2) Token-based candidate filtering + SequenceMatcher for near-duplicates

Usage:
  python -m preprocess.preprocess_gdelt.dedup_gdelt \
    --input <in.jsonl> --output <out.jsonl> [--threshold 0.90]
"""

from __future__ import annotations

import argparse
import json
import re
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, List, Set

RE_WHITESPACE = re.compile(r"\s+")
RE_PUNCT = re.compile(r"[\W_]+", flags=re.UNICODE)


def normalize_text(s: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ê·œí™”:
      - ì†Œë¬¸ì
      - ì¤„ë°”ê¿ˆ â†’ ê³µë°±
      - êµ¬ë‘ì  ì œê±°
      - ê³µë°± ì—¬ëŸ¬ ê°œ â†’ í•˜ë‚˜
    """
    if not s:
        return ""
    s2 = s.lower()
    s2 = s2.replace("\n", " ")
    s2 = RE_PUNCT.sub(" ", s2)
    s2 = RE_WHITESPACE.sub(" ", s2).strip()
    return s2


def row_key_text(row: Dict) -> str:
    """
    dedup ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  text:
      title + text ë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë³´ê³  ì²˜ë¦¬.
    """
    title = row.get("title") or ""
    text = row.get("text") or ""
    return normalize_text(title + " \n " + text)


def tokenise(s: str) -> List[str]:
    """
    ê°„ë‹¨í•œ í† í°í™”: ê³µë°± ê¸°ì¤€ split.
    ì´ë¯¸ normalize_text ë¥¼ ê±°ì³ ì•ŒíŒŒë²³/ìˆ«ì/ê³µë°± ì •ë„ë§Œ ë‚¨ì•„ìˆë‹¤.
    """
    if not s:
        return []
    return s.split()


def is_near_duplicate_with_candidates(
    s: str,
    candidates_idx: List[int],
    kept_texts: List[str],
    threshold: float,
) -> bool:
    """
    í›„ë³´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ì„œë§Œ SequenceMatcherë¥¼ ëŒë¦¬ë©°,
    threshold ì´ìƒì´ë©´ near-duplicateë¡œ ê°„ì£¼.
    """
    for idx in candidates_idx:
        c = kept_texts[idx]

        # ê¸¸ì´ê°€ ë„ˆë¬´ ë‹¤ë¥´ë©´ êµ³ì´ SequenceMatcher ëŒë¦´ í•„ìš” ì—†ìŒ (ê°„ë‹¨í•œ í”„ë¦¬í•„í„°)
        if abs(len(s) - len(c)) > max(200, int(0.5 * max(len(s), len(c)))):
            continue

        r = SequenceMatcher(None, s, c).ratio()
        if r >= threshold:
            return True
    return False


def dedup_jsonl(
    input_path: Path,
    output_path: Path,
    threshold: float = 0.90,
    max_tokens_for_index: int = 8,
) -> Dict:
    """
    GDELT ì „ì²˜ë¦¬ JSONL íŒŒì¼ì—ì„œ near-duplicateë¥¼ ì œê±°í•œë‹¤.

    - 1ë‹¨ê³„: exact-text dedup
        ê°™ì€ normalize_text(title+text)ë¥¼ ê°€ì§„ í–‰ì€ ë°”ë¡œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ìŠ¤í‚µ.
    - 2ë‹¨ê³„: token-based candidate í•„í„° + SequenceMatcher
        ì™„ì „ ë™ì¼ì€ ì•„ë‹ˆì§€ë§Œ, ë§¤ìš° ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ë¥¼ threshold ê¸°ì¤€ìœ¼ë¡œ ì œê±°.

    params
    -------
    threshold: SequenceMatcher similarity threshold (0~1).
    max_tokens_for_index:
        í•œ ë¬¸ì„œì— ëŒ€í•´ì„œ ì—­ìƒ‰ì¸ì— ë“±ë¡/ì¡°íšŒì— ì‚¬ìš©í•  í† í° ìˆ˜ ìƒí•œ.
    """
    kept_texts: List[str] = []  # ì •ê·œí™”ëœ ì „ì²´ í…ìŠ¤íŠ¸
    kept_tokens: List[Set[str]] = []  # ì¸ë±ì‹±ì— ì‚¬ìš©ëœ í† í° ì§‘í•©
    inverted_index: Dict[str, Set[int]] = {}  # token -> {kept index}

    # ğŸ”¥ exact-text dedup ìš©: ì •ê·œí™”ëœ text â†’ ì²« ë²ˆì§¸ ì¸ë±ìŠ¤
    exact_text_index: Dict[str, int] = {}

    kept_count = 0
    total = 0
    duplicates_near = 0
    duplicates_exact = 0

    with (
        input_path.open("r", encoding="utf-8") as infile,
        output_path.open("w", encoding="utf-8") as outfile,
    ):
        for line in infile:
            total += 1
            line = line.strip()
            if not line:
                continue
            try:
                row = json.loads(line)
            except Exception:
                # ê¹¨ì§„ ë¼ì¸ì€ ìŠ¤í‚µ
                continue

            s = row_key_text(row)

            # ---------- 1) exact-text dedup ----------
            if s in exact_text_index:
                # text(ì œëª©+ë³¸ë¬¸)ê¹Œì§€ ì™„ì „íˆ ê°™ì€ ê²½ìš° â†’ ë¬´ì¡°ê±´ ì¤‘ë³µ ì²˜ë¦¬
                duplicates_exact += 1
                continue

            if not s:
                # í…ìŠ¤íŠ¸ê°€ ì „í˜€ ì—†ìœ¼ë©´ ë¹„êµê°€ ì–´ë ¤ìš°ë‹ˆ ê·¸ëƒ¥ ì‚´ë¦°ë‹¤.
                outfile.write(json.dumps(row, ensure_ascii=False) + "\n")
                kept_texts.append("")
                kept_tokens.append(set())
                exact_text_index[""] = kept_count
                kept_count += 1
                continue

            toks = tokenise(s)
            if not toks:
                # í† í°í™”ê°€ ì•ˆë˜ë©´(ì „ë¶€ ìˆ«ì/ê³µë°± ë“±) ê·¸ëƒ¥ ì‚´ë¦°ë‹¤.
                outfile.write(json.dumps(row, ensure_ascii=False) + "\n")
                kept_texts.append(s)
                kept_tokens.append(set())
                exact_text_index[s] = kept_count
                kept_count += 1
                continue

            # ---------- 2) token ê¸°ë°˜ í›„ë³´ ìˆ˜ì§‘ ----------
            tokens_for_index = toks[:max_tokens_for_index]
            candidate_indices: Set[int] = set()
            for t in tokens_for_index:
                idx_set = inverted_index.get(t)
                if idx_set:
                    candidate_indices.update(idx_set)

            # í›„ë³´ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ SequenceMatcherë¡œ near-duplicate ê²€ì‚¬
            if candidate_indices:
                if is_near_duplicate_with_candidates(
                    s, list(candidate_indices), kept_texts, threshold
                ):
                    duplicates_near += 1
                    continue

            # ---------- keep ----------
            outfile.write(json.dumps(row, ensure_ascii=False) + "\n")
            cur_idx = kept_count

            kept_texts.append(s)
            tokset = set(tokens_for_index)
            kept_tokens.append(tokset)

            # exact-text ì¸ë±ìŠ¤ ê°±ì‹ 
            exact_text_index[s] = cur_idx

            # ì—­ìƒ‰ì¸ ê°±ì‹ 
            for t in tokset:
                if t not in inverted_index:
                    inverted_index[t] = set()
                inverted_index[t].add(cur_idx)

            kept_count += 1

    return {
        "total": total,
        "kept": kept_count,
        "duplicates_exact": duplicates_exact,
        "duplicates_near": duplicates_near,
        "output": str(output_path),
    }


def main(argv=None):
    ap = argparse.ArgumentParser(
        description=(
            "Deduplicate a GDELT-preprocessed JSONL file "
            "(exact + near-duplicates, faster version)."
        )
    )
    ap.add_argument("--input", "-i", required=True, help="Input JSONL path")
    ap.add_argument("--output", "-o", required=True, help="Output JSONL path")
    ap.add_argument(
        "--threshold",
        "-t",
        type=float,
        default=0.90,
        help="Similarity threshold (0-1) for SequenceMatcher (near-duplicates)",
    )
    ap.add_argument(
        "--max-tokens",
        type=int,
        default=8,
        help="Number of tokens per document to index for candidate search (default: 8)",
    )
    args = ap.parse_args(argv)

    inp = Path(args.input)
    out = Path(args.output)
    if not inp.exists():
        raise SystemExit(f"Input not found: {inp}")

    stats = dedup_jsonl(
        inp, out, threshold=args.threshold, max_tokens_for_index=args.max_tokens
    )
    print(
        "Dedup complete:"
        f" total={stats['total']},"
        f" kept={stats['kept']},"
        f" exact_dups={stats['duplicates_exact']},"
        f" near_dups={stats['duplicates_near']}"
    )


if __name__ == "__main__":
    main()
